{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df95f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손글씨자료를 딥러닝(뉴럴 네트워크:출력값이 많이 나옴)으로 학습해 예측\n",
    "#!pip install tensorflow-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07c8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723efe9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a202a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fe590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind=\"train\"): # load_mnist라는 함수 생성 / 파일을 4개 읽을 것이므로 경로와 kind의 변수로 train으로 설정\n",
    "    labels_path = os.path.join(path, \"%s-labels-idx1-ubyte\"%kind) #label file\n",
    "    images_path = os.path.join(path, \"%s-images-idx3-ubyte\"%kind) #image file / idx3 : 3차원\n",
    "    # label에 관련된 처리\n",
    "    with open(labels_path,\"rb\") as la_path:\n",
    "        magic,n = struct.unpack(\">II\", la_path.read(8)) # 두개를 읽어올 것임 / la_path.read(8) : 8바이트로 읽음\n",
    "        labels = np.fromfile(la_path,dtype=np.uint8) #test용 label을 읽을 수 있고 train용 label을 읽을 수도 있음\n",
    "    # image에 관련된 처리\n",
    "    with open(images_path, \"rb\") as img_path:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",img_path.read(16))\n",
    "        images = np.fromfile(img_path,dtype=np.uint8).reshape(len(labels),28**2)\n",
    "        images = ((images/255)-0.5)*2 #나눠서 0에서 1사이로 바꿔준다 / -0.5 : -0.5에서 0.5사이 / *2를 함으로써 -1과 1사이를 수행하도록 해서 표준화(standard scaling)\n",
    "    # 두개의 처리가 모두 끝나면 return\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6b2f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 784\n",
      "50000 784\n",
      "10000 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist(\"./\",kind=\"train\") #주피터와 파일이 같은 경로에 있어야 함\n",
    "X_test, y_test = load_mnist(\"./\",kind=\"t10k\")\n",
    "X_valid, y_valid = X_train[50000:,:], y_train[50000:]\n",
    "X_train, y_train = X_train[:50000,:], y_train[:50000]\n",
    "print(X_valid.shape[0],X_valid.shape[1]) #valid, train 재조정\n",
    "print(X_train.shape[0],X_train.shape[1]) #X_train이 총 60,000개 \n",
    "print(X_test.shape[0],X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e28fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train,axis=0) # 다시 정규화\n",
    "std_val = np.std(X_train)\n",
    "X_train_centered = (X_train-mean_vals)/std_val #표준화\n",
    "X_valid_centered = (X_valid-mean_vals)/std_val #X_valid도 전처리\n",
    "X_test_centered = (X_test-mean_vals)/std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa2a82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train) #범주형으로 10000,01000,00100 이렇게 바꿈\n",
    "y_valid_onehot = tf.keras.utils.to_categorical(y_valid)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test)\n",
    "y_train_onehot[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03b7bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9810d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#구조물\n",
    "model = tf.keras.models.Sequential() #Sequential정보를 만들어놓고 add함\n",
    "\n",
    "#layer가 3개니까 model도 3개\n",
    "model.add(tf.keras.layers.Dense(units = 50, #Denser는 일반적인 layer\n",
    "                                input_dim = X_train_centered.shape[1],\n",
    "                                kernel_initializer = 'glorot_uniform', #'glorot_uniform' : 균일하게 나갈 것 \n",
    "                                bias_initializer = 'zeros',\n",
    "                                activation = 'tanh')) #'tanh' : 다음에 어떻게 보낼지는 layer에게 줘라\n",
    "model.add(tf.keras.layers.Dense(units = 50, \n",
    "                                input_dim = 50, #784개의 입력을 받고 출력이 50개라 50개로 연결하므로 50이라 적어줌 (50보다 적어도 됌)\n",
    "                                kernel_initializer = 'glorot_uniform',\n",
    "                                bias_initializer = 'zeros',\n",
    "                                activation = 'tanh'))\n",
    "model.add(tf.keras.layers.Dense(units = 10, #출력에 맞게끔 줄임(답과 연결시켜야 하므로) \n",
    "                                input_dim = 50, # 이전 layer이 50개로 들어옴\n",
    "                                kernel_initializer = 'glorot_uniform',\n",
    "                                bias_initializer = 'zeros',\n",
    "                                activation = 'softmax')) #근사값을 들어 %를 사용해 확률적으로 답을 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefe97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() #tensorflow한 뒤 summary하면 편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cbbb09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.0001,decay=1e-7,momentum=0.9)\n",
    "model.compile(optimizer=sgd_optimizer, loss=\"categorical_crossentropy\") #optimizer두개를 결합 -> compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d9201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "704/704 [==============================] - 1s 967us/step - loss: 1.6461 - val_loss: 1.2222\n",
      "Epoch 2/50\n",
      "704/704 [==============================] - 1s 845us/step - loss: 1.0412 - val_loss: 0.9223\n",
      "Epoch 3/50\n",
      "704/704 [==============================] - 1s 804us/step - loss: 0.8282 - val_loss: 0.7736\n",
      "Epoch 4/50\n",
      "704/704 [==============================] - 1s 844us/step - loss: 0.7088 - val_loss: 0.6809\n",
      "Epoch 5/50\n",
      "704/704 [==============================] - 1s 762us/step - loss: 0.6305 - val_loss: 0.6171\n",
      "Epoch 6/50\n",
      "704/704 [==============================] - 1s 790us/step - loss: 0.5747 - val_loss: 0.5704\n",
      "Epoch 7/50\n",
      "704/704 [==============================] - 1s 772us/step - loss: 0.5327 - val_loss: 0.5348\n",
      "Epoch 8/50\n",
      "704/704 [==============================] - 1s 775us/step - loss: 0.5000 - val_loss: 0.5065\n",
      "Epoch 9/50\n",
      "704/704 [==============================] - 1s 816us/step - loss: 0.4736 - val_loss: 0.4837\n",
      "Epoch 10/50\n",
      "704/704 [==============================] - 1s 972us/step - loss: 0.4519 - val_loss: 0.4647\n",
      "Epoch 11/50\n",
      "704/704 [==============================] - 1s 971us/step - loss: 0.4335 - val_loss: 0.4488\n",
      "Epoch 12/50\n",
      "704/704 [==============================] - 1s 988us/step - loss: 0.4178 - val_loss: 0.4350\n",
      "Epoch 13/50\n",
      "704/704 [==============================] - 1s 998us/step - loss: 0.4042 - val_loss: 0.4232\n",
      "Epoch 14/50\n",
      "704/704 [==============================] - 1s 998us/step - loss: 0.3922 - val_loss: 0.4128\n",
      "Epoch 15/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.3816 - val_loss: 0.4035\n",
      "Epoch 16/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.3720 - val_loss: 0.3954\n",
      "Epoch 17/50\n",
      "704/704 [==============================] - 1s 970us/step - loss: 0.3634 - val_loss: 0.3881\n",
      "Epoch 18/50\n",
      "704/704 [==============================] - 1s 926us/step - loss: 0.3555 - val_loss: 0.3813\n",
      "Epoch 19/50\n",
      "704/704 [==============================] - 1s 972us/step - loss: 0.3483 - val_loss: 0.3752\n",
      "Epoch 20/50\n",
      "704/704 [==============================] - 1s 948us/step - loss: 0.3416 - val_loss: 0.3696\n",
      "Epoch 21/50\n",
      "704/704 [==============================] - 1s 948us/step - loss: 0.3354 - val_loss: 0.3643\n",
      "Epoch 22/50\n",
      "704/704 [==============================] - 1s 969us/step - loss: 0.3296 - val_loss: 0.3596\n",
      "Epoch 23/50\n",
      "704/704 [==============================] - 1s 998us/step - loss: 0.3243 - val_loss: 0.3551\n",
      "Epoch 24/50\n",
      "704/704 [==============================] - 1s 973us/step - loss: 0.3192 - val_loss: 0.3510\n",
      "Epoch 25/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.3144 - val_loss: 0.3473\n",
      "Epoch 26/50\n",
      "704/704 [==============================] - 1s 946us/step - loss: 0.3099 - val_loss: 0.3433\n",
      "Epoch 27/50\n",
      "704/704 [==============================] - 1s 958us/step - loss: 0.3056 - val_loss: 0.3400\n",
      "Epoch 28/50\n",
      "704/704 [==============================] - 1s 961us/step - loss: 0.3015 - val_loss: 0.3367\n",
      "Epoch 29/50\n",
      "704/704 [==============================] - 1s 946us/step - loss: 0.2977 - val_loss: 0.3335\n",
      "Epoch 30/50\n",
      "704/704 [==============================] - 1s 972us/step - loss: 0.2939 - val_loss: 0.3305\n",
      "Epoch 31/50\n",
      "704/704 [==============================] - 1s 970us/step - loss: 0.2904 - val_loss: 0.3276\n",
      "Epoch 32/50\n",
      "704/704 [==============================] - 1s 995us/step - loss: 0.2870 - val_loss: 0.3250\n",
      "Epoch 33/50\n",
      "704/704 [==============================] - 1s 903us/step - loss: 0.2837 - val_loss: 0.3223\n",
      "Epoch 34/50\n",
      "704/704 [==============================] - 1s 925us/step - loss: 0.2805 - val_loss: 0.3198\n",
      "Epoch 35/50\n",
      "704/704 [==============================] - 1s 926us/step - loss: 0.2775 - val_loss: 0.3175\n",
      "Epoch 36/50\n",
      "704/704 [==============================] - 1s 987us/step - loss: 0.2745 - val_loss: 0.3151\n",
      "Epoch 37/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2717 - val_loss: 0.3130\n",
      "Epoch 38/50\n",
      "704/704 [==============================] - 1s 958us/step - loss: 0.2689 - val_loss: 0.3108\n",
      "Epoch 39/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2663 - val_loss: 0.3086\n",
      "Epoch 40/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2637 - val_loss: 0.3066\n",
      "Epoch 41/50\n",
      "704/704 [==============================] - 1s 944us/step - loss: 0.2612 - val_loss: 0.3048\n",
      "Epoch 42/50\n",
      "704/704 [==============================] - 1s 973us/step - loss: 0.2587 - val_loss: 0.3029\n",
      "Epoch 43/50\n",
      "704/704 [==============================] - 1s 965us/step - loss: 0.2563 - val_loss: 0.3010\n",
      "Epoch 44/50\n",
      "704/704 [==============================] - 1s 923us/step - loss: 0.2540 - val_loss: 0.2992\n",
      "Epoch 45/50\n",
      "704/704 [==============================] - 1s 949us/step - loss: 0.2517 - val_loss: 0.2974\n",
      "Epoch 46/50\n",
      "704/704 [==============================] - 1s 948us/step - loss: 0.2495 - val_loss: 0.2957\n",
      "Epoch 47/50\n",
      "704/704 [==============================] - 1s 970us/step - loss: 0.2474 - val_loss: 0.2942\n",
      "Epoch 48/50\n",
      "704/704 [==============================] - 1s 971us/step - loss: 0.2453 - val_loss: 0.2926\n",
      "Epoch 49/50\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2432 - val_loss: 0.2909\n",
      "Epoch 50/50\n",
      "704/704 [==============================] - 1s 969us/step - loss: 0.2412 - val_loss: 0.2893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e2024858b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만들어진 model을 fitting\n",
    "model.fit(X_train_centered, y_train_onehot, batch_size=64,epochs=50, verbose=1,validation_split=0.1) # validation_split로 중간중간마다 안정화되는 방향으로 틀어야 함()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234dc1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered,verbose=0)\n",
    "y_train_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8def67b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bddf914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46546\n",
      "0.93092\n"
     ]
    }
   ],
   "source": [
    "#총 predict결과\n",
    "total_predicts = np.sum(y_train==y_train_pred,axis=0)\n",
    "print(total_predicts)\n",
    "train_res = total_predicts/y_train.shape[0]\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40172a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered,verbose=0)\n",
    "y_test_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f048f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7e7545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9307\n",
      "0.9307\n"
     ]
    }
   ],
   "source": [
    "total_predicts = np.sum(y_test==y_test_pred,axis=0)\n",
    "print(total_predicts)\n",
    "test_res = total_predicts/y_test.shape[0]\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a17c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5 6\n",
      "33 4 6\n",
      "66 6 7\n",
      "92 9 4\n",
      "124 7 4\n",
      "149 2 4\n",
      "187 5 8\n",
      "193 9 4\n",
      "195 3 5\n",
      "211 5 7\n",
      "233 8 7\n",
      "241 9 8\n",
      "247 4 2\n",
      "259 6 0\n",
      "261 5 1\n",
      "290 8 4\n",
      "300 4 6\n",
      "313 3 5\n",
      "320 9 7\n",
      "321 2 7\n",
      "340 5 3\n",
      "341 6 4\n",
      "352 5 0\n",
      "358 7 9\n",
      "362 2 7\n",
      "389 9 7\n",
      "403 8 9\n",
      "406 5 0\n",
      "412 5 3\n",
      "435 8 7\n",
      "444 2 8\n",
      "445 6 0\n",
      "448 9 8\n",
      "449 3 5\n",
      "478 5 8\n",
      "479 9 3\n",
      "495 8 0\n",
      "502 5 3\n",
      "507 3 5\n",
      "531 3 6\n",
      "543 8 3\n",
      "551 7 1\n",
      "565 4 9\n",
      "578 3 2\n",
      "582 8 2\n",
      "591 8 3\n",
      "613 2 8\n",
      "619 1 8\n",
      "627 9 4\n",
      "628 3 9\n",
      "629 2 6\n",
      "658 7 4\n",
      "659 2 8\n",
      "684 7 3\n",
      "691 8 4\n",
      "707 4 9\n",
      "714 8 5\n",
      "717 0 6\n",
      "720 5 8\n",
      "728 2 8\n",
      "740 4 9\n",
      "741 2 8\n",
      "760 4 9\n",
      "781 8 5\n",
      "791 5 9\n",
      "839 8 3\n",
      "844 8 7\n",
      "857 5 3\n",
      "882 9 7\n",
      "898 7 8\n",
      "924 2 7\n",
      "938 3 5\n",
      "939 2 0\n",
      "944 3 5\n",
      "947 8 9\n",
      "950 7 2\n",
      "959 4 5\n",
      "965 6 0\n",
      "982 3 8\n",
      "1003 5 8\n",
      "1014 6 5\n",
      "1017 6 2\n",
      "1032 5 8\n",
      "1039 7 9\n",
      "1044 6 8\n",
      "1050 2 6\n",
      "1062 3 7\n",
      "1068 8 4\n",
      "1082 5 3\n",
      "1101 8 2\n",
      "1107 9 3\n",
      "1112 4 6\n",
      "1114 3 8\n",
      "1119 7 2\n",
      "1124 8 7\n",
      "1128 3 7\n",
      "1131 5 4\n",
      "1173 7 9\n",
      "1181 6 1\n",
      "1182 6 8\n",
      "1191 0 4\n",
      "1192 9 4\n",
      "1194 7 9\n",
      "1198 8 4\n",
      "1200 8 3\n",
      "1202 8 5\n",
      "1204 3 8\n",
      "1206 7 2\n",
      "1224 2 6\n",
      "1226 7 2\n",
      "1232 9 4\n",
      "1234 8 5\n",
      "1242 4 9\n",
      "1247 9 0\n",
      "1248 8 5\n",
      "1256 2 3\n",
      "1260 7 1\n",
      "1283 7 2\n",
      "1289 5 9\n",
      "1291 3 5\n",
      "1299 5 7\n",
      "1315 3 5\n",
      "1319 8 3\n",
      "1325 8 6\n",
      "1326 7 2\n",
      "1337 2 6\n",
      "1364 8 2\n",
      "1378 5 6\n",
      "1391 4 9\n",
      "1393 5 3\n",
      "1402 2 7\n",
      "1410 2 6\n",
      "1429 9 4\n",
      "1433 8 3\n",
      "1444 6 7\n",
      "1466 5 0\n",
      "1467 5 9\n",
      "1476 5 3\n",
      "1500 7 1\n",
      "1522 7 9\n",
      "1525 5 0\n",
      "1527 1 6\n",
      "1530 8 7\n",
      "1549 4 6\n",
      "1553 9 3\n",
      "1559 9 3\n",
      "1569 6 7\n",
      "1581 7 9\n",
      "1587 6 5\n",
      "1609 2 6\n",
      "1621 0 6\n",
      "1634 4 7\n",
      "1640 9 4\n",
      "1678 2 0\n",
      "1681 3 7\n",
      "1686 8 5\n",
      "1696 2 9\n",
      "1709 9 5\n",
      "1717 8 0\n",
      "1718 7 3\n",
      "1722 2 4\n",
      "1732 9 5\n",
      "1737 5 2\n",
      "1751 4 2\n",
      "1754 7 2\n",
      "1759 8 6\n",
      "1765 3 5\n",
      "1772 7 4\n",
      "1774 8 5\n",
      "1782 8 7\n",
      "1790 2 9\n",
      "1800 6 4\n",
      "1813 8 5\n",
      "1850 8 9\n",
      "1855 8 9\n",
      "1857 6 4\n",
      "1868 1 4\n",
      "1878 8 3\n",
      "1901 9 4\n",
      "1911 5 6\n",
      "1917 5 8\n",
      "1926 3 5\n",
      "1938 4 6\n",
      "1941 7 4\n",
      "1942 8 5\n",
      "1952 9 5\n",
      "1968 8 1\n",
      "1970 5 3\n",
      "1973 8 5\n",
      "1981 6 4\n",
      "1984 2 0\n",
      "2016 7 2\n",
      "2033 0 4\n",
      "2035 5 3\n",
      "2043 4 8\n",
      "2044 2 7\n",
      "2052 8 6\n",
      "2053 4 9\n",
      "2068 9 4\n",
      "2070 7 9\n",
      "2093 8 1\n",
      "2098 2 0\n",
      "2099 8 9\n",
      "2109 3 4\n",
      "2110 2 8\n",
      "2118 6 0\n",
      "2129 9 2\n",
      "2130 4 9\n",
      "2135 6 1\n",
      "2138 2 8\n",
      "2148 4 9\n",
      "2182 1 2\n",
      "2185 0 5\n",
      "2186 2 0\n",
      "2189 9 1\n",
      "2266 1 6\n",
      "2272 8 0\n",
      "2293 9 6\n",
      "2299 2 7\n",
      "2326 0 5\n",
      "2369 5 9\n",
      "2371 4 9\n",
      "2378 0 2\n",
      "2380 9 0\n",
      "2387 9 1\n",
      "2393 8 3\n",
      "2394 4 9\n",
      "2395 8 3\n",
      "2406 9 1\n",
      "2408 3 9\n",
      "2414 9 4\n",
      "2422 6 4\n",
      "2449 0 5\n",
      "2460 5 8\n",
      "2473 1 8\n",
      "2488 2 4\n",
      "2526 5 3\n",
      "2534 3 5\n",
      "2556 5 8\n",
      "2573 5 8\n",
      "2574 5 9\n",
      "2598 8 2\n",
      "2607 7 1\n",
      "2610 2 8\n",
      "2648 9 5\n",
      "2650 8 5\n",
      "2654 6 1\n",
      "2670 5 8\n",
      "2695 7 4\n",
      "2698 5 3\n",
      "2713 0 8\n",
      "2730 7 4\n",
      "2756 3 2\n",
      "2770 3 6\n",
      "2771 4 9\n",
      "2780 2 3\n",
      "2802 2 0\n",
      "2810 5 0\n",
      "2832 5 3\n",
      "2836 4 9\n",
      "2850 5 3\n",
      "2866 6 4\n",
      "2877 4 7\n",
      "2896 8 0\n",
      "2906 3 5\n",
      "2919 5 9\n",
      "2921 3 2\n",
      "2925 5 0\n",
      "2927 3 2\n",
      "2945 3 7\n",
      "2953 3 5\n",
      "2990 8 9\n",
      "2995 6 8\n",
      "3005 9 1\n",
      "3060 9 7\n",
      "3073 1 3\n",
      "3110 3 5\n",
      "3114 4 6\n",
      "3117 5 9\n",
      "3130 6 0\n",
      "3133 4 9\n",
      "3136 7 1\n",
      "3138 3 0\n",
      "3145 5 9\n",
      "3157 5 7\n",
      "3160 9 4\n",
      "3164 6 2\n",
      "3167 3 1\n",
      "3186 8 2\n",
      "3189 7 4\n",
      "3193 3 2\n",
      "3206 8 3\n",
      "3240 9 3\n",
      "3269 6 5\n",
      "3280 2 7\n",
      "3284 8 7\n",
      "3289 8 7\n",
      "3316 7 4\n",
      "3329 7 2\n",
      "3330 2 3\n",
      "3333 7 9\n",
      "3336 5 7\n",
      "3369 9 1\n",
      "3381 3 8\n",
      "3422 6 0\n",
      "3436 2 1\n",
      "3468 5 4\n",
      "3475 3 7\n",
      "3490 4 9\n",
      "3503 9 1\n",
      "3516 8 5\n",
      "3520 6 4\n",
      "3533 4 9\n",
      "3549 3 2\n",
      "3558 5 0\n",
      "3559 8 5\n",
      "3567 8 5\n",
      "3573 7 4\n",
      "3597 9 3\n",
      "3598 1 8\n",
      "3604 7 0\n",
      "3618 9 7\n",
      "3629 8 3\n",
      "3662 8 5\n",
      "3681 2 5\n",
      "3702 5 3\n",
      "3716 9 3\n",
      "3718 4 9\n",
      "3726 4 9\n",
      "3727 8 5\n",
      "3730 7 9\n",
      "3751 7 1\n",
      "3757 8 3\n",
      "3767 7 2\n",
      "3776 5 8\n",
      "3780 4 2\n",
      "3796 2 1\n",
      "3801 6 0\n",
      "3806 5 9\n",
      "3808 7 3\n",
      "3811 2 3\n",
      "3817 2 8\n",
      "3820 9 7\n",
      "3836 7 4\n",
      "3838 7 1\n",
      "3848 7 2\n",
      "3853 6 5\n",
      "3855 5 0\n",
      "3862 2 3\n",
      "3869 9 4\n",
      "3876 2 8\n",
      "3893 5 6\n",
      "3902 5 3\n",
      "3906 1 3\n",
      "3926 9 3\n",
      "3941 4 6\n",
      "3943 3 5\n",
      "3946 2 8\n",
      "3951 8 7\n",
      "3954 8 9\n",
      "3962 3 2\n",
      "3985 9 4\n",
      "4000 9 4\n",
      "4002 3 5\n",
      "4017 4 9\n",
      "4027 7 9\n",
      "4063 6 5\n",
      "4065 0 6\n",
      "4072 5 3\n",
      "4075 8 0\n",
      "4076 5 8\n",
      "4078 9 7\n",
      "4093 9 4\n",
      "4126 8 1\n",
      "4131 5 3\n",
      "4140 8 2\n",
      "4145 8 5\n",
      "4152 5 1\n",
      "4154 9 4\n",
      "4156 2 8\n",
      "4163 9 5\n",
      "4176 2 4\n",
      "4177 5 4\n",
      "4199 7 9\n",
      "4201 1 7\n",
      "4205 2 1\n",
      "4211 6 5\n",
      "4212 1 3\n",
      "4224 9 7\n",
      "4238 7 9\n",
      "4239 6 5\n",
      "4248 2 8\n",
      "4284 9 5\n",
      "4289 2 7\n",
      "4297 7 3\n",
      "4300 5 9\n",
      "4302 5 8\n",
      "4306 3 7\n",
      "4315 5 4\n",
      "4317 3 7\n",
      "4344 9 7\n",
      "4355 5 9\n",
      "4356 5 8\n",
      "4369 9 4\n",
      "4374 5 6\n",
      "4380 8 5\n",
      "4400 7 4\n",
      "4405 9 4\n",
      "4425 9 4\n",
      "4427 2 8\n",
      "4433 7 3\n",
      "4435 3 7\n",
      "4449 6 0\n",
      "4451 2 5\n",
      "4497 8 9\n",
      "4498 7 9\n",
      "4500 9 1\n",
      "4523 8 3\n",
      "4540 7 9\n",
      "4567 4 9\n",
      "4571 6 2\n",
      "4575 4 2\n",
      "4601 8 4\n",
      "4615 2 4\n",
      "4620 6 0\n",
      "4635 3 5\n",
      "4639 8 9\n",
      "4724 8 5\n",
      "4731 8 7\n",
      "4735 9 4\n",
      "4740 3 5\n",
      "4751 4 6\n",
      "4761 9 1\n",
      "4785 3 8\n",
      "4807 8 3\n",
      "4814 6 0\n",
      "4823 9 4\n",
      "4837 7 2\n",
      "4852 8 6\n",
      "4874 9 0\n",
      "4876 2 4\n",
      "4879 8 4\n",
      "4880 0 8\n",
      "4886 7 1\n",
      "4890 8 6\n",
      "4910 9 4\n",
      "4915 5 8\n",
      "4939 2 3\n",
      "4950 2 3\n",
      "4956 8 4\n",
      "4966 7 1\n",
      "4978 8 5\n",
      "4990 3 1\n",
      "5001 9 4\n",
      "5009 9 4\n",
      "5054 3 5\n",
      "5065 8 1\n",
      "5067 3 2\n",
      "5068 4 1\n",
      "5078 3 2\n",
      "5140 3 4\n",
      "5210 9 7\n",
      "5246 7 2\n",
      "5331 1 6\n",
      "5409 4 6\n",
      "5523 9 7\n",
      "5600 7 9\n",
      "5617 4 9\n",
      "5620 7 9\n",
      "5634 2 3\n",
      "5642 1 5\n",
      "5662 5 3\n",
      "5677 4 6\n",
      "5678 8 5\n",
      "5691 4 1\n",
      "5718 0 5\n",
      "5734 3 9\n",
      "5735 5 6\n",
      "5749 8 6\n",
      "5821 5 3\n",
      "5842 4 7\n",
      "5862 5 3\n",
      "5867 5 3\n",
      "5887 7 2\n",
      "5888 4 0\n",
      "5891 5 6\n",
      "5912 3 0\n",
      "5913 5 3\n",
      "5922 5 3\n",
      "5935 3 8\n",
      "5936 4 9\n",
      "5945 3 8\n",
      "5955 3 8\n",
      "5957 5 8\n",
      "5973 3 8\n",
      "5975 4 9\n",
      "5985 5 8\n",
      "6002 6 4\n",
      "6020 6 4\n",
      "6023 3 5\n",
      "6035 2 0\n",
      "6037 4 9\n",
      "6042 5 8\n",
      "6043 5 8\n",
      "6045 3 8\n",
      "6059 3 9\n",
      "6071 9 3\n",
      "6081 9 8\n",
      "6091 9 0\n",
      "6112 9 0\n",
      "6124 9 0\n",
      "6157 9 0\n",
      "6166 9 3\n",
      "6172 9 0\n",
      "6173 9 0\n",
      "6324 5 6\n",
      "6347 8 6\n",
      "6391 2 6\n",
      "6400 0 6\n",
      "6421 3 2\n",
      "6425 6 2\n",
      "6426 0 6\n",
      "6428 0 2\n",
      "6432 3 8\n",
      "6505 9 0\n",
      "6555 8 7\n",
      "6560 9 3\n",
      "6568 9 4\n",
      "6569 3 9\n",
      "6571 9 7\n",
      "6574 2 6\n",
      "6597 0 7\n",
      "6598 5 7\n",
      "6603 8 7\n",
      "6614 2 0\n",
      "6625 8 4\n",
      "6632 9 5\n",
      "6641 8 5\n",
      "6651 0 5\n",
      "6706 5 7\n",
      "6718 9 4\n",
      "6721 2 4\n",
      "6740 9 0\n",
      "6744 2 8\n",
      "6746 5 4\n",
      "6755 8 7\n",
      "6765 8 6\n",
      "6775 5 8\n",
      "6784 9 4\n",
      "6785 2 4\n",
      "6796 2 7\n",
      "6817 9 4\n",
      "6847 6 4\n",
      "6906 2 6\n",
      "6926 6 4\n",
      "7002 2 8\n",
      "7035 8 5\n",
      "7049 0 6\n",
      "7121 8 9\n",
      "7130 3 9\n",
      "7198 8 9\n",
      "7216 0 6\n",
      "7220 8 3\n",
      "7233 3 7\n",
      "7265 8 3\n",
      "7338 4 9\n",
      "7372 5 3\n",
      "7426 9 4\n",
      "7432 7 2\n",
      "7434 4 8\n",
      "7451 5 6\n",
      "7472 2 9\n",
      "7487 9 4\n",
      "7498 5 3\n",
      "7539 2 1\n",
      "7595 3 8\n",
      "7637 2 3\n",
      "7672 5 8\n",
      "7724 2 3\n",
      "7797 5 6\n",
      "7800 3 2\n",
      "7812 1 8\n",
      "7821 3 2\n",
      "7839 1 8\n",
      "7842 5 4\n",
      "7849 3 2\n",
      "7850 5 4\n",
      "7851 6 0\n",
      "7858 3 2\n",
      "7859 5 4\n",
      "7870 5 4\n",
      "7876 2 4\n",
      "7886 2 4\n",
      "7888 5 4\n",
      "7918 5 8\n",
      "7921 8 1\n",
      "7945 2 6\n",
      "8020 1 8\n",
      "8062 5 8\n",
      "8072 5 3\n",
      "8081 4 6\n",
      "8094 2 1\n",
      "8095 4 8\n",
      "8165 2 4\n",
      "8246 3 5\n",
      "8258 8 6\n",
      "8272 3 5\n",
      "8277 3 5\n",
      "8279 8 6\n",
      "8293 3 8\n",
      "8296 3 8\n",
      "8308 3 5\n",
      "8332 9 7\n",
      "8339 8 6\n",
      "8375 7 9\n",
      "8408 8 6\n",
      "8410 8 6\n",
      "8426 9 4\n",
      "8453 5 3\n",
      "8520 4 9\n",
      "8522 8 6\n",
      "8553 5 3\n",
      "9007 3 8\n",
      "9009 7 2\n",
      "9010 2 8\n",
      "9013 5 6\n",
      "9015 7 2\n",
      "9019 7 2\n",
      "9024 7 2\n",
      "9026 9 4\n",
      "9036 7 2\n",
      "9045 7 2\n",
      "9071 1 8\n",
      "9182 3 9\n",
      "9211 4 9\n",
      "9280 8 5\n",
      "9316 8 9\n",
      "9422 5 3\n",
      "9446 2 4\n",
      "9465 5 3\n",
      "9482 5 3\n",
      "9534 7 9\n",
      "9544 9 7\n",
      "9587 9 4\n",
      "9595 2 8\n",
      "9614 3 5\n",
      "9624 3 8\n",
      "9634 0 3\n",
      "9642 9 7\n",
      "9643 1 7\n",
      "9664 2 7\n",
      "9679 6 3\n",
      "9692 9 7\n",
      "9698 6 5\n",
      "9700 2 6\n",
      "9716 2 0\n",
      "9719 5 0\n",
      "9729 5 6\n",
      "9732 8 5\n",
      "9744 8 1\n",
      "9745 4 2\n",
      "9749 5 6\n",
      "9751 2 0\n",
      "9752 2 0\n",
      "9768 2 0\n",
      "9770 5 0\n",
      "9777 5 0\n",
      "9779 2 0\n",
      "9780 8 1\n",
      "9808 9 4\n",
      "9811 2 8\n",
      "9839 2 7\n",
      "9856 9 4\n",
      "9858 6 3\n",
      "9867 2 8\n",
      "9879 0 2\n",
      "9883 5 1\n",
      "9888 6 0\n",
      "9890 9 4\n",
      "9892 8 6\n",
      "9893 2 8\n",
      "9905 3 9\n",
      "9925 3 2\n",
      "9940 6 0\n",
      "9941 5 6\n",
      "9943 3 8\n",
      "9944 3 9\n",
      "9953 6 0\n",
      "9959 8 7\n",
      "9970 5 3\n",
      "9975 3 2\n",
      "9982 5 2\n",
      "9986 3 8\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for i, x in enumerate(y_test):\n",
    "    if x != y_test_pred[i]:\n",
    "        print(i,x,y_test_pred[i])\n",
    "        n+=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c1e94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "image = np.reshape(X_test[684],[28,28])\n",
    "plt.imshow(image,cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3167b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ee9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,111,946\n",
      "Trainable params: 1,111,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#28*28짜리 이미지를 가져와서\n",
    "from tensorflow.keras import layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(5,5), padding='valid', activation='relu', input_shape=(28,28,1))) #32로 깊이가 깊어진만큼 가로세로사이즈가 줄어듦\n",
    "model.add(layers.MaxPool2D(2,2)) #줄이는 것 : 가로세로가 반절로 줄어듦(깊이는 변하지 않음)\n",
    "model.add(layers.Conv2D(64,(5,5), padding='valid', activation='relu')) #깊이를 두배로 늘림(그만큼 부피가 유지되느라 가로세로가 줄어듦)\n",
    "model.add(layers.MaxPool2D(2,2)) #그림이 점점 작아짐\n",
    "model.add(layers.Flatten()) #4*4*64 = 1024(reshape과 비슷)\n",
    "model.add(layers.Dense(1024,activation='relu')) #학습을 함\n",
    "model.add(layers.Dropout(0.5)) #dense에 영향을 미쳐서 50% 효율로 껐다켰다하면서 막음\n",
    "model.add(layers.Dense(10,activation='softmax')) #마지막 dense는 대답을 하는 학습을 함\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7152b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2ab1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:230 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 784)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8bbf4e58dd4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fitting결과를 history에 담아보기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_centered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_centered\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:230 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 784)\n"
     ]
    }
   ],
   "source": [
    "# fitting결과를 history에 담아보기\n",
    "history = model.fit(X_train_centered, y_train_onehot, batch_size=64,epochs=20,validation_data=(X_valid_centered,y_valid_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,21),history.history['loss'],label=\"loss\")\n",
    "plt.plot(np.arange(1,21),history.history['val_loss'],label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8116447",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.predict(X_test_centered[:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a329dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1dd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608f56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6b39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
